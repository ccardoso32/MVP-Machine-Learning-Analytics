{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8b955b",
   "metadata": {},
   "source": [
    "\n",
    "# MVP: *Machine Learning & Analytics* — Breast Cancer (Classificação)\n",
    "**Autor:** _CAIO CARDOSO DE SOUZA_  \n",
    "**Data:** 15/09/2025  \n",
    "**Matrícula:** 4052025000946  \n",
    "**Dataset:** [Breast Cancer Wisconsin (Diagnostic) — scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f5905",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Escopo, objetivo e definição do problema\n",
    "\n",
    "**Contexto:** O câncer de mama é uma das principais causas de mortalidade entre mulheres. A detecção precoce aumenta a chance de tratamento eficaz e reduz custos.  \n",
    "**Objetivo:** Construir e comparar modelos de *machine learning* para classificar tumores como **malignos (0)** ou **benignos (1)** a partir de 30 variáveis numéricas extraídas de imagens histopatológicas.  \n",
    "**Tipo de tarefa:** Classificação binária supervisionada.  \n",
    "**Área:** Dados tabulares biomédicos.  \n",
    "**Valor de negócio/usuário:** Apoio à decisão clínica (triagem). *Este notebook é educacional e não substitui diagnóstico médico.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e417580",
   "metadata": {},
   "source": [
    "## 2. Reprodutibilidade e ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ec4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 2: Setup básico e reprodutibilidade ===\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn core\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, cross_val_score,\n",
    "                                     GridSearchCV)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# modelos\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier)\n",
    "\n",
    "# métricas/plots\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score, classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "\n",
    "# reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# visualização de dataframes sem quebra\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0], \"| Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b7cd8",
   "metadata": {},
   "source": [
    "### 2.2 Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a82aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_classification(y_true, y_pred, proba=None):\n",
    "    # Retorna dicionário com métricas principais\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"roc_auc\": roc_auc_score(y_true, proba[:,1]) if proba is not None else np.nan\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def summarize_cv(results_dict):\n",
    "    # Imprime média e desvio dos scores de CV e retorna ranking\n",
    "    lines = []\n",
    "    for name, scores in results_dict.items():\n",
    "        lines.append((name, scores.mean(), scores.std()))\n",
    "    lines = sorted(lines, key=lambda t: t[1], reverse=True)\n",
    "    for name, m, s in lines:\n",
    "        print(f\"{name}: {m:.3f} ({s:.3f})\")\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7398517",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dados: carga, entendimento e qualidade\n",
    "\n",
    "**Fonte:** *scikit-learn* (`load_breast_cancer`). 569 instâncias, 30 variáveis numéricas.  \n",
    "**Ética/licença:** dataset público; sem dados pessoais identificáveis. Uso educacional.  \n",
    "**Prevenção de *data leakage*:** divisão treino/teste **antes** de qualquer transformação; todas as transformações ajustadas **apenas no treino** via `Pipeline/ColumnTransformer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 3: Carga e entendimento dos dados ===\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "print(\"Formato (linhas, colunas):\", df.shape)\n",
    "print(\"Distribuição de classes {maligno, benigno}:\", dict(zip(data.target_names, np.bincount(data.target))))\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc3cad",
   "metadata": {},
   "source": [
    "### 3.1 Análise exploratória resumida (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desc = X.describe().T[[\"mean\",\"std\",\"min\",\"max\"]]\n",
    "display(desc.head(8))\n",
    "\n",
    "# distribuição da classe\n",
    "_, counts = np.unique(y, return_counts=True)\n",
    "plt.bar([\"maligno (0)\", \"benigno (1)\"], counts)\n",
    "plt.title(\"Distribuição de classes\"); plt.show()\n",
    "\n",
    "# correlação rápida (subconjunto para visualização)\n",
    "corr_subset = X.iloc[:, :12].corr()\n",
    "plt.imshow(corr_subset, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar(); plt.title(\"Correlação (12 primeiras features)\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f8382",
   "metadata": {},
   "source": [
    "## 4. Definição do target, variáveis e divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac64982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 4: Target e split 80/20 ===\n",
    "target = \"target\"\n",
    "features = X.columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[features], y, test_size=0.20, stratify=y, random_state=SEED\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n",
    "print(\"Proporção de classes (train):\\n\", y_train.value_counts(normalize=True).round(3))\n",
    "print(\"Proporção de classes (test):\\n\", y_test.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de18d7",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Seleção de variáveis (*Feature Selection*) — opcional, orientada por importância\n",
    "\n",
    "**Por quê agora?** Após o *split*, antes de montar os *pipelines*. Assim garantimos que a seleção é aprendida **só** no treino, evitando *data leakage* e permitindo que os *preprocessors* usem apenas as colunas escolhidas.\n",
    "\n",
    "**Critério adaptativo:**  \n",
    "1) Ajustamos um **RandomForest** (robusto a escala) no treino com *imputação* para obter `feature_importances_`.  \n",
    "2) Se a razão `max_importance / min_importance < 3`, entendemos que os \"pesos\" são relativamente parecidos → **mantemos todas** as 30 variáveis (preferência por interpretabilidade e simplicidade).  \n",
    "3) Caso contrário, mantemos o menor conjunto de variáveis que acumule **≥95%** da importância total (com no mínimo 12 features), e reportamos a lista final.  \n",
    "4) Como checagem, também calculamos o ranking univariado (`SelectKBest(f_classif)`) e mostramos a interseção com as escolhidas.\n",
    "\n",
    "\n",
    "> **Nota didática:** embora este dataset seja pequeno (569 amostras, 30 variáveis),\n",
    "incluímos a etapa de *feature selection* **por fins educacionais**, para documentar o processo,\n",
    "mostrar como evitar *data leakage* e observar o impacto na validação cruzada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 5: Feature Selection (condicional) ===\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# 1) RF só com imputação (sem escala)\n",
    "num_cols_all = X_train.columns.tolist()\n",
    "rf_fs = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=400, random_state=SEED, n_jobs=-1))\n",
    "])\n",
    "rf_fs.fit(X_train[num_cols_all], y_train)\n",
    "importances = rf_fs.named_steps[\"rf\"].feature_importances_\n",
    "imp_ser = pd.Series(importances, index=num_cols_all).sort_values(ascending=False)\n",
    "\n",
    "# Plot top-20 para diagnóstico\n",
    "ax = imp_ser.head(20).plot(kind=\"barh\", figsize=(8,6))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Importância de features (RF) — Top 20\")\n",
    "plt.show()\n",
    "\n",
    "ratio = imp_ser.max() / max(imp_ser.min(), 1e-12)\n",
    "cum = imp_ser.cumsum()\n",
    "\n",
    "# 2) Decisão adaptativa\n",
    "if ratio < 3.0:\n",
    "    decision = \"keep_all\"\n",
    "    final_num_cols = num_cols_all  # pesos semelhantes → manter todas\n",
    "else:\n",
    "    cutoff_idx = np.argmax(cum.values >= 0.95)  # menor índice que atinge 95%\n",
    "    cutoff_idx = max(cutoff_idx, 11)            # garantir pelo menos 12 colunas\n",
    "    final_num_cols = imp_ser.index[:cutoff_idx+1].tolist()\n",
    "    decision = \"select_subset\"\n",
    "\n",
    "print(f\"Razão max/min de importância: {ratio:.2f}\")\n",
    "print(f\"Decisão: {decision}\")\n",
    "print(f\"Nº de colunas finais: {len(final_num_cols)} / {len(num_cols_all)}\")\n",
    "\n",
    "# 3) Checagem com SelectKBest (mesmo tamanho do conjunto escolhido)\n",
    "k_sel = len(final_num_cols)\n",
    "skb = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std\", StandardScaler()),\n",
    "    (\"skb\", SelectKBest(score_func=f_classif, k=k_sel))\n",
    "])\n",
    "skb.fit(X_train[num_cols_all], y_train)\n",
    "mask = skb.named_steps[\"skb\"].get_support()\n",
    "skb_cols = X_train.columns[mask].tolist()\n",
    "\n",
    "inter = set(final_num_cols).intersection(skb_cols)\n",
    "print(f\"Interseção RF-Importância vs SelectKBest: {len(inter)} colunas\")\n",
    "print(sorted(list(inter))[:10], \"...\")\n",
    "\n",
    "# Vamos registrar o racional em texto para o relatório\n",
    "if decision == \"keep_all\":\n",
    "    rationale_text = (\n",
    "        \"Os pesos relativos estão próximos (max/min < 3). Mantivemos as 30 variáveis \"\n",
    "        \"para preservar interpretabilidade e evitar risco de remover sinais úteis; \"\n",
    "        \"models baseados em árvore e regressão logística lidam bem com esse tamanho.\"\n",
    "    )\n",
    "else:\n",
    "    rationale_text = (\n",
    "        f\"Os pesos variaram bastante (max/min = {ratio:.2f}). Mantivemos \"\n",
    "        f\"{len(final_num_cols)} variáveis que acumulam ≥95% da importância (RF). \"\n",
    "        \"Isso tende a reduzir variância e tempo de treino, com baixo custo de viés.\"\n",
    "    )\n",
    "\n",
    "rationale_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d533d",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Efeito da *feature selection*: antes vs. depois (CV estratificada 10-fold)\n",
    "\n",
    "Para fins **didáticos**, comparamos rapidamente 3 modelos (Regressão Logística, RandomForest, KNN)\n",
    "**antes** (todas as 30 variáveis) e **depois** (conjunto `final_num_cols`) da seleção,\n",
    "sempre dentro de *pipelines* para evitar *leakage*. Reportamos média e desvio-padrão da **acurácia (CV=10)**\n",
    "e o **delta** entre as versões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 5.1: Comparação antes/depois da FS (CV=10) ===\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Pipelines \"antes\" (todas as colunas)\n",
    "all_cols = X_train.columns.tolist()\n",
    "pre_none_all = ColumnTransformer([(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), all_cols)])\n",
    "pre_std_all  = ColumnTransformer([(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), all_cols)])\n",
    "\n",
    "models_before = {\n",
    "    \"LogReg\": Pipeline([(\"pre\", pre_std_all),  (\"m\", LogisticRegression(max_iter=1000, random_state=SEED))]),\n",
    "    \"RF\"    : Pipeline([(\"pre\", pre_none_all), (\"m\", RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1))]),\n",
    "    \"KNN\"   : Pipeline([(\"pre\", pre_std_all),  (\"m\", KNeighborsClassifier())])\n",
    "}\n",
    "\n",
    "# Pipelines \"depois\" (features finais)\n",
    "pre_none_fs = ColumnTransformer([(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), final_num_cols)])\n",
    "pre_std_fs  = ColumnTransformer([(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), final_num_cols)])\n",
    "\n",
    "models_after = {\n",
    "    \"LogReg\": Pipeline([(\"pre\", pre_std_fs),  (\"m\", LogisticRegression(max_iter=1000, random_state=SEED))]),\n",
    "    \"RF\"    : Pipeline([(\"pre\", pre_none_fs), (\"m\", RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1))]),\n",
    "    \"KNN\"   : Pipeline([(\"pre\", pre_std_fs),  (\"m\", KNeighborsClassifier())])\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name in [\"LogReg\", \"RF\", \"KNN\"]:\n",
    "    sc_before = cross_val_score(models_before[name], X_train, y_train, cv=cv10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    sc_after  = cross_val_score(models_after[name],  X_train, y_train, cv=cv10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    rows.append({\n",
    "        \"modelo\": name,\n",
    "        \"antes_mean\": sc_before.mean(), \"antes_std\": sc_before.std(),\n",
    "        \"depois_mean\": sc_after.mean(), \"depois_std\": sc_after.std(),\n",
    "        \"delta_mean\": sc_after.mean() - sc_before.mean()\n",
    "    })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows).set_index(\"modelo\").round(4)\n",
    "print(\"Comparação (acurácia CV=10) — antes vs. depois da FS\")\n",
    "display(cmp_df)\n",
    "\n",
    "# Visualização simples (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "ax = cmp_df[[\"antes_mean\",\"depois_mean\"]].plot(kind=\"bar\", figsize=(8,4))\n",
    "plt.title(\"Acurácia média (CV=10) — antes vs. depois da FS\")\n",
    "plt.ylabel(\"acurácia\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6fff5",
   "metadata": {},
   "source": [
    "## 6. Tratamento dos dados e Pipeline de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e841501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 6: Pré-processamento com as features finais ===\n",
    "num_cols = final_num_cols  # usar decisão do bloco anterior\n",
    "\n",
    "numeric_pipe_none = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "numeric_pipe_std  = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                                    (\"scaler\", StandardScaler())])\n",
    "numeric_pipe_min  = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                                    (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "preproc_none = ColumnTransformer([(\"num\", numeric_pipe_none, num_cols)])\n",
    "preproc_std  = ColumnTransformer([(\"num\", numeric_pipe_std,  num_cols)])\n",
    "preproc_min  = ColumnTransformer([(\"num\", numeric_pipe_min,  num_cols)])\n",
    "\n",
    "print(f\"Colunas usadas nos pipelines: {len(num_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 5: Pré-processamento com ColumnTransformer ===\n",
    "num_cols = X_train.columns.tolist()\n",
    "\n",
    "numeric_pipe_none = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "numeric_pipe_std = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_pipe_min = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "preproc_none = ColumnTransformer([(\"num\", numeric_pipe_none, num_cols)])\n",
    "preproc_std  = ColumnTransformer([(\"num\", numeric_pipe_std,  num_cols)])\n",
    "preproc_min  = ColumnTransformer([(\"num\", numeric_pipe_min,  num_cols)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616341d0",
   "metadata": {},
   "source": [
    "## 6. Baseline e modelos candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f062bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Bloco 6: Definição dos modelos e montagem das pipelines ===\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"CART\": DecisionTreeClassifier(random_state=SEED),\n",
    "    \"NB\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True, random_state=SEED),\n",
    "    \"Bag\": BaggingClassifier(random_state=SEED),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=200, random_state=SEED),\n",
    "    \"ET\": ExtraTreesClassifier(n_estimators=200, random_state=SEED),\n",
    "    \"Ada\": AdaBoostClassifier(n_estimators=150, random_state=SEED),\n",
    "    \"GB\": GradientBoostingClassifier(n_estimators=200, random_state=SEED),\n",
    "}\n",
    "\n",
    "voting = VotingClassifier(estimators=[(\"lr\", models[\"LR\"]),\n",
    "                                      (\"rf\", models[\"RF\"]),\n",
    "                                      (\"gb\", models[\"GB\"])],\n",
    "                          voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "pipelines = {}\n",
    "# baseline\n",
    "pipelines[\"Dummy\"] = Pipeline([(\"pre\", preproc_none), (\"model\", DummyClassifier(strategy=\"most_frequent\"))])\n",
    "\n",
    "for tag, pre in [(\"orig\", preproc_none), (\"padr\", preproc_std), (\"norm\", preproc_min)]:\n",
    "    for mname, m in {**models, \"Vot\": voting}.items():\n",
    "        pipelines[f\"{mname}-{tag}\"] = Pipeline([(\"pre\", pre), (\"model\", m)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87711c1f",
   "metadata": {},
   "source": [
    "### 6.1 Treino e avaliação rápida (CV + boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11230e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_scores = {}\n",
    "names, results = [], []\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_scores[name] = scores\n",
    "    names.append(name); results.append(scores)\n",
    "\n",
    "_ = summarize_cv(cv_scores)\n",
    "\n",
    "# Boxplot comparativo\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Comparação de Modelos (orig/padr/norm) - Acurácia (CV)\")\n",
    "plt.ylabel(\"Accuracy (CV)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6415a8",
   "metadata": {},
   "source": [
    "## 7. Validação e Otimização de Hiperparâmetros (GridSearch KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7509036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_knn = {\n",
    "    \"model__n_neighbors\": list(range(1, 22, 2)),\n",
    "    \"model__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "}\n",
    "\n",
    "gs_results = {}\n",
    "for tag, pre in [(\"orig\", preproc_none), (\"padr\", preproc_std), (\"norm\", preproc_min)]:\n",
    "    pipe_knn = Pipeline([(\"pre\", pre), (\"model\", KNeighborsClassifier())])\n",
    "    gs = GridSearchCV(pipe_knn, param_grid_knn, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    gs_results[tag] = gs\n",
    "    print(f\"[KNN-{tag}] melhor score (CV): {gs.best_score_:.4f} | melhores params: {gs.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921fe6e",
   "metadata": {},
   "source": [
    "## 8. Avaliação final, análise de erros e limitações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleção do melhor por CV\n",
    "best_name, best_mean = None, -np.inf\n",
    "for name, scores in cv_scores.items():\n",
    "    if scores.mean() > best_mean:\n",
    "        best_name, best_mean = name, scores.mean()\n",
    "\n",
    "best_pipe = pipelines[best_name]\n",
    "print(f\"Melhor por CV: {best_name} | mean={best_mean:.4f}\")\n",
    "\n",
    "# treino no conjunto de treino completo\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "# avaliação em teste\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "y_proba = best_pipe.predict_proba(X_test) if hasattr(best_pipe, \"predict_proba\") else None\n",
    "\n",
    "metrics = evaluate_classification(y_test, y_pred, y_proba)\n",
    "print(\"\\nMétricas (teste):\", metrics)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# matriz de confusão\n",
    "ConfusionMatrixDisplay.from_estimator(best_pipe, X_test, y_test, display_labels=data.target_names)\n",
    "plt.title(f\"Matriz de confusão — {best_name} (teste)\")\n",
    "plt.show()\n",
    "\n",
    "# importância de features se disponível\n",
    "if hasattr(best_pipe.named_steps[\"model\"], \"feature_importances_\"):\n",
    "    importances = best_pipe.named_steps[\"model\"].feature_importances_\n",
    "    imp = pd.Series(importances, index=X.columns).sort_values(ascending=False).head(15)\n",
    "    imp.plot(kind=\"barh\"); plt.gca().invert_yaxis()\n",
    "    plt.title(\"Top 15 importâncias (árvore/ensemble)\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670957c7",
   "metadata": {},
   "source": [
    "## 9. Refit com todo o dataset e salvando artefatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit no dataset inteiro (produção)\n",
    "best_pipe.fit(X, y)\n",
    "\n",
    "# (Opcional) salvar pipeline\n",
    "# import joblib; joblib.dump(best_pipe, \"breast_cancer_best_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf1459",
   "metadata": {},
   "source": [
    "## 10. Predição em novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acfc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulação de 3 novas instâncias\n",
    "novos = X.sample(3, random_state=SEED).reset_index(drop=True)\n",
    "pred = best_pipe.predict(novos)\n",
    "pred_labels = [data.target_names[i] for i in pred]\n",
    "print(\"Predições (0=maligno, 1=benigno):\", pred.tolist(), \"->\", pred_labels)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
